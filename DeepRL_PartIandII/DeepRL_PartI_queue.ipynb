{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ssxiwwi66Se3"
      },
      "source": [
        "Install dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "idJX2upP6O8W",
        "outputId": "af0cfcf7-ac74-4fa7-bf34-265362c01bba"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "E: Could not open lock file /var/lib/dpkg/lock-frontend - open (13: Permission denied)\n",
            "E: Unable to acquire the dpkg frontend lock (/var/lib/dpkg/lock-frontend), are you root?\n",
            "Requirement already satisfied: gym[box2d]==0.17.* in /home/yukie/anaconda3/envs/APA/lib/python3.9/site-packages (0.17.3)\n",
            "Requirement already satisfied: pyvirtualdisplay==0.2.* in /home/yukie/anaconda3/envs/APA/lib/python3.9/site-packages (0.2.5)\n",
            "Requirement already satisfied: PyOpenGL==3.1.* in /home/yukie/anaconda3/envs/APA/lib/python3.9/site-packages (3.1.5)\n",
            "Requirement already satisfied: PyOpenGL-accelerate==3.1.* in /home/yukie/anaconda3/envs/APA/lib/python3.9/site-packages (3.1.5)\n",
            "Requirement already satisfied: cloudpickle<1.7.0,>=1.2.0 in /home/yukie/anaconda3/envs/APA/lib/python3.9/site-packages (from gym[box2d]==0.17.*) (1.6.0)\n",
            "Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in /home/yukie/anaconda3/envs/APA/lib/python3.9/site-packages (from gym[box2d]==0.17.*) (1.5.0)\n",
            "Requirement already satisfied: numpy>=1.10.4 in /home/yukie/anaconda3/envs/APA/lib/python3.9/site-packages (from gym[box2d]==0.17.*) (1.20.3)\n",
            "Requirement already satisfied: scipy in /home/yukie/anaconda3/envs/APA/lib/python3.9/site-packages (from gym[box2d]==0.17.*) (1.7.1)\n",
            "Requirement already satisfied: box2d-py~=2.3.5 in /home/yukie/anaconda3/envs/APA/lib/python3.9/site-packages (from gym[box2d]==0.17.*) (2.3.8)\n",
            "Requirement already satisfied: EasyProcess in /home/yukie/anaconda3/envs/APA/lib/python3.9/site-packages (from pyvirtualdisplay==0.2.*) (0.3)\n",
            "Requirement already satisfied: future in /home/yukie/anaconda3/envs/APA/lib/python3.9/site-packages (from pyglet<=1.5.0,>=1.4.0->gym[box2d]==0.17.*) (0.18.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install gym pyvirtualdisplay > /dev/null 2>&1\n",
        "!apt-get install -y xvfb python-opengl ffmpeg > /dev/null 2>&1\n",
        "# install required system dependencies\n",
        "!apt-get install -y xvfb x11-utils\n",
        "\n",
        "# install required python dependencies (might need to install additional gym extras depending)\n",
        "!pip install gym[box2d]==0.17.* pyvirtualdisplay==0.2.* PyOpenGL==3.1.* PyOpenGL-accelerate==3.1.*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-1O4fl6bwGG_",
        "outputId": "b555e1ae-2aea-4089-e052-d4ecf0483257"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "True\n",
            "cuda:0\n"
          ]
        }
      ],
      "source": [
        "#@title Imports\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "from matplotlib.pyplot import figure\n",
        "\n",
        "import gym\n",
        "import torch\n",
        "from torch.utils import data\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.autograd as autograd\n",
        "import random\n",
        "from gym import wrappers\n",
        "import copy\n",
        "import gym\n",
        "from gym.wrappers import Monitor\n",
        "import glob\n",
        "import io\n",
        "import base64\n",
        "from IPython.display import HTML\n",
        "from IPython.display import Video\n",
        "from pyvirtualdisplay import Display\n",
        "from IPython import display as ipythondisplay\n",
        "import sys\n",
        "Running_in_colab = 'google.colab' in sys.modules\n",
        "\n",
        "display = Display(visible=0, size=(1400, 900))\n",
        "display.start()\n",
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "from PartI_lib import archs as archs\n",
        "from PartI_lib import performance_evaluation as eval\n",
        "from PartI_lib import train_loop as tl\n",
        "from PartI_lib import my_tools as mt\n",
        "# !mkdir Checkpoints\n",
        "import time\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "IIeU54J8wXkI"
      },
      "outputs": [],
      "source": [
        "#@title Deep RL Replay Buffer\n",
        "\n",
        "class ReplayBuffer:\n",
        "    def __init__(self, size, input_shape,dev):\n",
        "        self.size = size\n",
        "        self.counter = 0\n",
        "        self.state_buffer = torch.zeros((self.size, input_shape))\n",
        "        self.action_buffer = torch.zeros(self.size, dtype=torch.int64)\n",
        "        self.reward_buffer = torch.zeros(self.size)\n",
        "        self.new_state_buffer = torch.zeros((self.size, input_shape))\n",
        "        self.terminal_buffer = torch.zeros(self.size)\n",
        "        self.state_buffer.to(dev)\n",
        "        self.action_buffer.to(dev)\n",
        "        self.reward_buffer.to(dev)\n",
        "        self.new_state_buffer.to(dev)\n",
        "        self.terminal_buffer.to(dev)\n",
        "        self.dev=dev;\n",
        "\n",
        "\n",
        "\n",
        "    def store_tuples(self, state, action, reward, new_state, done):\n",
        "        idx = self.counter % self.size\n",
        "        self.state_buffer[idx] = state\n",
        "        self.action_buffer[idx] = action\n",
        "        self.reward_buffer[idx] = reward\n",
        "        self.new_state_buffer[idx] = new_state\n",
        "        self.terminal_buffer[idx] = done\n",
        "        self.counter += 1\n",
        "\n",
        "    def sample_buffer(self, batch_size):\n",
        "        max_buffer = min(self.counter, self.size)\n",
        "        batch = np.random.choice(max_buffer, batch_size, replace=False)\n",
        "        state_batch = self.state_buffer[batch].to(self.dev)\n",
        "        action_batch = self.action_buffer[batch].to(self.dev)\n",
        "        reward_batch = self.reward_buffer[batch].to(self.dev)\n",
        "        new_state_batch = self.new_state_buffer[batch].to(self.dev)\n",
        "        done_batch = self.terminal_buffer[batch].to(self.dev)\n",
        "\n",
        "        return state_batch, action_batch, reward_batch, new_state_batch, done_batch\n",
        "    def last_buffer(self, batch_size):\n",
        "        state_batch = self.state_buffer[-1:-batch_size].to(self.dev)\n",
        "        action_batch = self.action_buffer[-1:-batch_size].to(self.dev)\n",
        "        reward_batch = self.reward_buffer[-1:-batch_size].to(self.dev)\n",
        "        new_state_batch = self.new_state_buffer[-1:-batch_size].to(self.dev)\n",
        "        done_batch = self.terminal_buffer[-1:-batch_size].to(self.dev)\n",
        "\n",
        "        return state_batch, action_batch, reward_batch, new_state_batch, done_batch\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "cellView": "form",
        "id": "dVwFwJMNxoCw"
      },
      "outputs": [],
      "source": [
        "#@title DQN movie generation (for visual evaluation in Google Colab)\n",
        "\n",
        "def createMovie(Network,path,Filename):\n",
        "    env = gym.make(\"CartPole-v0\")\n",
        "    envX = wrappers.Monitor(env,path+'/'+Filename,force=True)\n",
        "\n",
        "    observation = envX.reset()\n",
        "\n",
        "    i=0\n",
        "    Network.eval()\n",
        "\n",
        "    while True:\n",
        "        envX.render()\n",
        "        \n",
        "        state=torch.Tensor(observation).to(device)\n",
        "      \n",
        "        action = Network.policy(state);\n",
        "             \n",
        "        observation, reward, done, info = envX.step(action.item()) \n",
        "        i=i+1;     \n",
        "        if done: \n",
        "          break;\n",
        "\n",
        "    envX.close()\n",
        "    env.close();\n",
        "    Network.train()\n",
        "    mp4list = glob.glob(path+Filename+'/*.mp4')\n",
        "    if len(mp4list) > 0:\n",
        "        mp4 = mp4list[0]\n",
        "        video = io.open(mp4, 'r+b').read()\n",
        "        encoded = base64.b64encode(video)\n",
        "        if(Running_in_colab):\n",
        "            ipythondisplay.display(HTML(data='''<video alt=\"test\" autoplay \n",
        "                    loop controls style=\"height: 400px;\">\n",
        "                    <source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\" />\n",
        "                </video>'''.format(encoded.decode('ascii'))))\n",
        "        else:\n",
        "            Video(mp4)\n",
        "    else: \n",
        "        print(\"Could not find video\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OsEv2cZLwy3e"
      },
      "source": [
        "Initialization and Parameters:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6yygleEXxEpG",
        "outputId": "6252a170-0e96-443e-f1eb-5fb7d078a71e"
      },
      "outputs": [
        {
          "ename": "TypeError",
          "evalue": "object of type 'int' has no len()",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipykernel_32558/2569897466.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0mll\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m                 \u001b[0mfile_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0march\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mn_layers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mBatchSize\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mexploration_threshold\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mexploration_threshold_min\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mexploration_decay\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdiscount_factor\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mLearningRate\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mLearningRateDecay\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m                 \u001b[0mfile_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_dir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_path_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfile_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m                 \u001b[0mmt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_dir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"Checkpoints\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/Documents/APA/APA_Assignment_3/DeepRL_PartIandII/PartI_lib/my_tools.py\u001b[0m in \u001b[0;36mset_name\u001b[0;34m(arch, n_layers, BatchSize, exploration_threshold, exploration_threshold_min, exploration_decay, discount_factor, LearningRate, LearningRateDecay)\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mset_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0march\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mn_layers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mBatchSize\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mexploration_threshold\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mexploration_threshold_min\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mexploration_decay\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdiscount_factor\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mLearningRate\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mLearningRateDecay\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_layers\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m         \u001b[0mnew_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0march\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\"_\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_layers\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"_\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBatchSize\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"_\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexploration_threshold\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'.'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"_\"\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m                 \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexploration_threshold_min\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'.'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"_\"\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: object of type 'int' has no len()"
          ]
        }
      ],
      "source": [
        "# Sim configuration\n",
        "env = gym.make(\"CartPole-v0\")\n",
        "spec = gym.spec(\"CartPole-v0\")\n",
        "\n",
        "\n",
        "\n",
        "inputs=4\n",
        "n_actions=2\n",
        "\n",
        "#hyper-parameters\n",
        "TotalEpisodes=2000\n",
        "MaxSteps=400\n",
        "FreezeCounter=25\n",
        "BatchSize=128\n",
        "exploration_threshold=1\n",
        "exploration_threshold_min=0.01\n",
        "# exploration_decay=0.002\n",
        "discount_factor=0.99\n",
        "SaveAtCounter=200\n",
        "LearningRateDecay=0.99\n",
        "\n",
        "\n",
        "#network DQN\n",
        "\n",
        "buffer = ReplayBuffer(1000000, inputs,device);\n",
        "\n",
        "for arch in [\"DQN\",\"DoubleDQN\",\"DoubleDuelingDQN\"]:\n",
        "    file_path__ = mt.create_dir(\"results\",arch)\n",
        "    for n_layers in [4]:#,2,6]:\n",
        "        policy_net, target_net = archs.archs(arch,inputs,n_actions,discount_factor,device,n_layers)\n",
        "        target_net.load_state_dict(policy_net.state_dict())\n",
        "        target_net.eval()\n",
        "        file_path_ = mt.create_dir(file_path__,\"N\"+str(n_layers)+\"_Layers\")\n",
        "        for LearningRate in [0.001, 0.0001]:\n",
        "            for exploration_decay in [0.004,0.02, 0.002]: \n",
        "                optimizer = torch.optim.Adam(policy_net.parameters(), lr=LearningRate)\n",
        "                scheduler = optim.lr_scheduler.ExponentialLR(optimizer,gamma=LearningRateDecay)\n",
        "                loss = torch.nn.MSELoss()\n",
        "                def trainModel():\n",
        "                    if buffer.counter < BatchSize:\n",
        "                        return 0.0\n",
        "                    \n",
        "                    state_batch, action_batch, reward_batch, new_state_batch, done_batch = buffer.sample_buffer(BatchSize)\n",
        "\n",
        "                    q_actual = torch.gather(policy_net(state_batch),1,action_batch.reshape(-1,1))\n",
        "                    with torch.no_grad():\n",
        "                        if(arch == \"DoubleDQN\" or arch == \"DoubleDuelingDQN\"):\n",
        "                            target = torch.argmax(policy_net(state_batch), -1).detach()\n",
        "                            q_max_next = target_net(new_state_batch).gather(1, target.unsqueeze(-1)).squeeze(-1)\n",
        "                        else:\n",
        "                            q_max_next = target_net(new_state_batch).max(1)[0].detach()\n",
        "                    q_target = (q_max_next * discount_factor)*(1-done_batch) + reward_batch\n",
        "\n",
        "                    ll=loss(q_actual, q_target.unsqueeze(1))\n",
        "\n",
        "                    # Optimize the model\n",
        "                    optimizer.zero_grad()\n",
        "                    ll.backward()\n",
        "                    for param in policy_net.parameters():\n",
        "                        param.grad.data.clamp_(-1, 1)\n",
        "                    optimizer.step()\n",
        "\n",
        "                    return ll.item();\n",
        "                file_name = mt.set_name(arch,n_layers,BatchSize,exploration_threshold,exploration_threshold_min,exploration_decay,discount_factor,LearningRate,LearningRateDecay)\n",
        "                file_path = mt.create_dir(file_path_,file_name)\n",
        "                mt.create_dir(file_path,\"Checkpoints\")\n",
        "                print(file_name)\n",
        "                bestNet, episodes, scores, events, avg_scores, avg_scores20, exploration, avg_scores100 = tl.train_loop(policy_net, target_net, env, device, TotalEpisodes, FreezeCounter, SaveAtCounter, createMovie, MaxSteps, exploration_threshold, exploration_decay, exploration_threshold_min, buffer, trainModel,file_path)\n",
        "\n",
        "                eval.performance_evaluation(file_path,episodes, scores, events, avg_scores, avg_scores20, exploration, avg_scores100)\n",
        "                eval.report(file_path,arch,BatchSize,exploration_threshold,exploration_threshold_min,exploration_decay,discount_factor,\n",
        "                            LearningRate,LearningRateDecay,episodes, scores, events, avg_scores, avg_scores20, exploration, n_layers,avg_scores100)\n",
        "                createMovie(bestNet,file_path,'bestNet')\n",
        "                time.sleep(1)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "DeepRL_PartI.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
