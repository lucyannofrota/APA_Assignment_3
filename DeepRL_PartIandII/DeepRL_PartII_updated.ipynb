{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PGYDUCOIAgVC"
      },
      "source": [
        "#Assignment DeepRL Part II - Version 2\n",
        "\n",
        "- The part of the code related to clonning the track was **not working as expected**.\n",
        "\n",
        "- The code is now updated to set a fixed seed to the number generator.\n",
        "\n",
        "- The seed is set by (STUDENTID1+STUDENTID2)%100 or STUDENTID1%100 so set the values **accordingly**.\n",
        "\n",
        "- When the UseClonedTrack flag is set to false, the seed uses time to set its value.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ssxiwwi66Se3"
      },
      "source": [
        "Install dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "idJX2upP6O8W"
      },
      "outputs": [],
      "source": [
        "!apt-get update\n",
        "!pip install gym pyvirtualdisplay > /dev/null 2>&1\n",
        "!apt-get install -y xvfb python-opengl ffmpeg > /dev/null 2>&1\n",
        "# install required system dependencies\n",
        "!apt-get install -y xvfb x11-utils\n",
        "\n",
        "# install required python dependencies (might need to install additional gym extras depending)\n",
        "!pip install gym[box2d]==0.17.* pyvirtualdisplay==0.2.* PyOpenGL==3.1.* PyOpenGL-accelerate==3.1.*\n",
        "\n",
        "!git clone https://github.com/openai/gym.git\n",
        "%cd gym\n",
        "!pip install -e .\n",
        "!pip install stable-baselines[mpi]\n",
        "!pip install stable-baselines3[extra]\n",
        "\n",
        "%cd .."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "473bp5sKwJTB"
      },
      "source": [
        "Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-1O4fl6bwGG_"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "from matplotlib.pyplot import figure\n",
        "\n",
        "import gym\n",
        "import torch \n",
        "from torch.utils import data\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.autograd as autograd\n",
        "import random\n",
        "from gym import wrappers\n",
        "import copy\n",
        "import gym\n",
        "from gym.wrappers import Monitor\n",
        "import glob\n",
        "import io\n",
        "import base64\n",
        "from IPython.display import HTML\n",
        "from pyvirtualdisplay import Display\n",
        "from IPython import display as ipythondisplay\n",
        "\n",
        "display = Display(visible=0, size=(1400, 900))\n",
        "display.start()\n",
        "# device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# from PartI_lib import archs as archs\n",
        "from PartI_lib import performance_evaluation as eval\n",
        "# from PartI_lib import train_loop as tl\n",
        "from PartI_lib import my_tools as mt\n",
        "\n",
        "\n",
        "###################################################\n",
        "###################################################\n",
        "###################################################\n",
        "STUDENTID1=2016116214;\n",
        "STUDENTID2=2016193815;\n",
        "###################################################\n",
        "###################################################\n",
        "###################################################\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iSvUkpCgwRL9"
      },
      "source": [
        "Deep RL Replay Buffer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IIeU54J8wXkI"
      },
      "outputs": [],
      "source": [
        "class ReplayBuffer:\n",
        "    def __init__(self, size):\n",
        "        self.size = size\n",
        "        self.counter = 0\n",
        "        self.state_buffer = []\n",
        "        self.action_buffer = []\n",
        "        self.reward_buffer = []\n",
        "        self.new_state_buffer = []\n",
        "        self.terminal_buffer = []\n",
        "        \n",
        "    def store_tuples(self, state, action, reward, new_state, done):\n",
        "        if len(self.state_buffer) < self.size:\n",
        "            self.state_buffer.append(None)\n",
        "            self.action_buffer.append(None)\n",
        "            self.reward_buffer.append(None)\n",
        "            self.new_state_buffer.append(None)\n",
        "            self.terminal_buffer.append(None)\n",
        "            \n",
        "        self.counter = self.counter % self.size\n",
        "        self.state_buffer[self.counter] = state\n",
        "        self.action_buffer[self.counter] = action\n",
        "        self.reward_buffer[self.counter] = reward\n",
        "        self.new_state_buffer[self.counter] = new_state\n",
        "        self.terminal_buffer[self.counter] = done\n",
        "        self.counter += 1\n",
        "\n",
        "    def sample_buffer(self, batch_size):\n",
        "        max_buffer = min(self.counter, self.size)\n",
        "        batch = np.random.choice(max_buffer, batch_size, replace=False)\n",
        "        state_batch=[];\n",
        "        action_batch=[];\n",
        "        reward_batch=[];\n",
        "        new_state_batch=[];\n",
        "        done_batch=[];\n",
        "        \n",
        "        for a in batch:\n",
        "            state_batch.append(self.state_buffer[a])\n",
        "            action_batch.append(self.action_buffer[a])\n",
        "            reward_batch.append(self.reward_buffer[a])\n",
        "            new_state_batch.append(self.new_state_buffer[a])\n",
        "            done_batch.append(self.terminal_buffer[a])\n",
        "        return state_batch, action_batch, reward_batch, new_state_batch, done_batch\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fG0T040FwczS"
      },
      "source": [
        "\n",
        "\n",
        "Default DQN arquitecture"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aGrl7mfpwgfM"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "\n",
        "def size_after_maxpool(img_size,kernel_size, stride):\n",
        "    ret = math.floor(1+(img_size-kernel_size)/stride)\n",
        "    return ret\n",
        "\n",
        "class DQN(nn.Module):\n",
        "\n",
        "    def __init__(self, inputs, outputs):\n",
        "        super(DQN, self).__init__()\n",
        "        \n",
        "        self.input_size=inputs;\n",
        "        self.output_size=outputs;\n",
        "        self.imgSize=96;\n",
        "        # self.size_a_maxpool = size_after_maxpool(96,3,2);\n",
        "        \n",
        "        \n",
        "        self.features = nn.Sequential(\n",
        "            nn.Conv2d(in_channels=self.input_size, \n",
        "                      out_channels=64, \n",
        "                      kernel_size=5,\n",
        "                      stride=1,\n",
        "                      padding=2),\n",
        "            nn.ReLU(),\n",
        "            # nn.MaxPool2d(kernel_size=3, stride=2),\n",
        "            # nn.Conv2d(in_channels=64, \n",
        "            #           out_channels=32, \n",
        "            #           kernel_size=5,\n",
        "            #           stride=1,\n",
        "            #           padding=2),\n",
        "            # nn.ReLU(),\n",
        "            nn.Conv2d(in_channels=64,\n",
        "                      out_channels=8, \n",
        "                      kernel_size=5,\n",
        "                      stride=1,\n",
        "                      padding=2),\n",
        "            nn.ReLU(),\n",
        "        )\n",
        "        \n",
        "        self.layers = nn.Sequential(\n",
        "            nn.Linear(in_features=self.imgSize*self.imgSize*8, out_features=64),            #Add code here for DQN \n",
        "            nn.ReLU(),\n",
        "            nn.Linear(in_features=64, out_features=self.output_size),            #Add code here for DQN \n",
        "        )\n",
        "        \n",
        "       \n",
        "        self.layersdueling = nn.Sequential(\n",
        "            #Add code here for Dueling \n",
        "        )\n",
        "\n",
        "\n",
        "        self.advantage = nn.Sequential(\n",
        "            #Add code here for Dueling \n",
        "        )\n",
        "        \n",
        "        self.value = nn.Sequential(\n",
        "           #Add code here for Dueling \n",
        "        )\n",
        "  \n",
        "\n",
        "###### Dueling configuration \n",
        "                                                    #############\n",
        "###########    ############    #################    # advantage #           \n",
        "#         #    #          #    #               # -> #############    \n",
        "#   IMG   # -> # features # -> # layersdueling # -> #############\n",
        "#         #    #  (CONVs) #    #     (FCs)     #    #   value   #\n",
        "###########    ############    #################    #############\n",
        " \n",
        "\n",
        "    def forward(self, x):\n",
        "    \n",
        "        if(useDueling):\n",
        "            x=self.features(x)\n",
        "            x = x.view(x.size(0), -1)\n",
        "            x = self.layersdueling(x)\n",
        "            advantage = self.advantage(x)\n",
        "            value     = self.value(x)\n",
        "            return value + advantage  - advantage.mean()\n",
        "        else:\n",
        "            x=self.features(x)\n",
        "            x = x.view(x.size(0), -1)\n",
        "            return self.layers(x)\n",
        "\n",
        "    def policy(self,state):\n",
        "       with torch.no_grad():\n",
        "            return self.__call__(state.unsqueeze(0)).argmax()\n",
        "     \n",
        "    def getPolicy(self,state,eps_threshold):\n",
        "        sample = random.random()\n",
        "        if sample > eps_threshold:\n",
        "            with torch.no_grad():\n",
        "                return self.__call__(state.unsqueeze(0)).argmax()\n",
        "        else:\n",
        "            return  torch.tensor(random.randrange(self.output_size), device=device, dtype=torch.long)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e9Cz-vtcwv1s"
      },
      "source": [
        "OpenAI Environment CarRacing-v0\n",
        "\n",
        " \n",
        "###    Description:\n",
        "        Easiest continuous control task to learn from pixels, a top-down racing environment.\n",
        "###    Observation:\n",
        "        Type: Image (96x96x3)\n",
        "        Some indicators are shown at the bottom of the window along with the state RGB buffer. From left to right: the true speed, four ABS sensors, the steering wheel position and gyroscope.\n",
        "###    Actions:\n",
        "        Discrete control is reasonable in this environment as well, on/off discretization is fine.\n",
        "###    Reward:\n",
        "        The reward is -0.1 every frame and +1000/N for every track tile visited, where N is the total number of tiles visited in the track. For example, if you have finished in 732 frames, your reward is 1000 - 0.1*732 = 926.8 points.\n",
        "###    Starting State:\n",
        "        The tracks are randomly generated (switch to True the UseClonedTrack flag to activate this behavior) for each episode.\n",
        "###    Episode Termination:\n",
        "        Car is too faraway from the track.\n",
        "        Car accumulated a negative reward after given iterations (see ResetCounter and InitCounter).\n",
        "        Episode length is greater than 1000.\n",
        "        All tiles are visited.\n",
        "###     Solved Requirements:\n",
        "        The game is solved when the agent consistently gets 900+ points.  \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "og-Pb_QrxWf_"
      },
      "source": [
        "DQN movie generation (for visual evaluation in Google Colab)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dVwFwJMNxoCw"
      },
      "outputs": [],
      "source": [
        "def createMovie(Network,path,Filename,UseClone):\n",
        "  \n",
        "    env1 = gym.make(\"CarRacing-v0\")\n",
        "    envX = wrappers.Monitor(env1,path+'/'+Filename,force=True)\n",
        "  \n",
        "    \n",
        "    if(UseClone):\n",
        "       envX.seed((STUDENTID1+STUDENTID2)%100);\n",
        "    else:\n",
        "       envX.seed(time.time_ns());\n",
        "    new_state = envX.reset()\n",
        "\n",
        "\n",
        "    \n",
        "    stackedStateX=[np.rollaxis(new_state, 2, 0).copy() for i in range(FrameStack)];\n",
        "    state =torch.from_numpy(np.reshape(np.array(stackedStateX),(FrameSize*FrameStack,96,96)) ).float().to(device)\n",
        "\n",
        "\n",
        "    i=0\n",
        "    Network.eval()\n",
        "    resetStatus=0;   \n",
        "\n",
        "    while True:\n",
        "        envX.render()\n",
        "        \n",
        "        action = Network.policy(state);\n",
        "        reward=0\n",
        "        for x in range(ControlSteps):\n",
        "            new_state, r, done, _ = envX.step(action_space[action.item()])\n",
        "            reward+=r\n",
        "            if(done):\n",
        "                break\n",
        "        if reward<0 and i>InitCounter:\n",
        "            resetStatus=resetStatus+1;\n",
        "        else:\n",
        "            resetStatus=0\n",
        "        \n",
        "        stackedStateX.pop(0)\n",
        "        \n",
        " \n",
        "        stackedStateX.append(np.rollaxis(new_state, 2, 0).copy())\n",
        "        state=torch.from_numpy(np.reshape(np.array(stackedStateX),(FrameSize*FrameStack,96,96))).float().to(device)\n",
        "        i=i+1;     \n",
        "        if done or resetStatus>=ResetCounter: \n",
        "          break;\n",
        "\n",
        "    envX.close()\n",
        "    env1.close();\n",
        "    Network.train()\n",
        "    mp4list = glob.glob(path+'/'+Filename+'/*.mp4')\n",
        "    if len(mp4list) > 0:\n",
        "        mp4 = mp4list[0]\n",
        "        video = io.open(mp4, 'r+b').read()\n",
        "        encoded = base64.b64encode(video)\n",
        "        # ipythondisplay.display(HTML(data='''<video alt=\"test\" autoplay \n",
        "        #         loop controls style=\"height: 400px;\">\n",
        "        #         <source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\" />\n",
        "        #      </video>'''.format(encoded.decode('ascii'))))\n",
        "    else: \n",
        "        print(\"Could not find video\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LE6SfnBL4ZOh"
      },
      "source": [
        "Dummy Net with random policy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pTsrsjcExD1m"
      },
      "outputs": [],
      "source": [
        "class RandomNet(nn.Module):\n",
        "\n",
        "  def __init__(self,outputs):\n",
        "      super(RandomNet, self).__init__()        \n",
        "      self.output_size=outputs;\n",
        "      \n",
        "\n",
        "  def forward(self, x):\n",
        "      return  x\n",
        "\n",
        "  def policy(self,state):\n",
        "      return  torch.tensor([[random.randrange(self.output_size)]], device=device, dtype=torch.long)\n",
        "     \n",
        "    \n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5zH08XItLDFB"
      },
      "source": [
        "Parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pZO6lE44LEn5"
      },
      "outputs": [],
      "source": [
        "action_space    = [\n",
        "            (-1  , 1  , 0.2), (0, 1, 0.2), (1   , 1  , 0.2), #           Action Space Structure\n",
        "            (-1  , 1  ,   0), (0, 1,   0), (1   , 1  ,   0), #        (Steering Wheel, Gas, Break)\n",
        "            (-1  , 0  , 0.2), (0, 0, 0.2), (1   , 0  , 0.2), # Range        -1~1       0~1   0~1\n",
        "            (-1  , 0  ,   0), (0, 0,   0), (1   , 0  ,   0)];\n",
        "            # (-0.5, 0  ,   0),              (0.5 , 0  ,   0),\n",
        "            # (-0.5, 0.5,   0),              (0.5 , 0.5,   0),\n",
        "            # (-0.5, 0  , 0.5),              (0.5 , 0  , 0.5)];\n",
        "\n",
        " \n",
        "n_actions=len(action_space)\n",
        "\n",
        "#hyper-parameters\n",
        "TotalEpisodes=101;\n",
        "MaxSteps=2000;\n",
        "ControlSteps=3; # number of iterations the same action is executed in the environment \n",
        "FrameStack=3 # number of consecutive frames used to represent the state\n",
        "FreezeCounter=25; # clone the model every X episodes\n",
        "BatchSize=64;\n",
        "exploration_threshold=1\n",
        "exploration_threshold_min=0.01\n",
        "exploration_decay=0.997\n",
        "discount_factor=0.95\n",
        "LearningRate=0.0001\n",
        "LearningRateDecay=1\n",
        " \n",
        "SaveAtCounter=25 # save model or video at every X episodes\n",
        "ResetCounter=50 # maximum number of experiences with bad performance\n",
        "InitCounter=20 # minimum number of experiences per episode\n",
        "FrameSize=3; # 3 if RGB 1 if grayscale\n",
        "\n",
        "\n",
        "# use these flags to enable Double DQN and Duelinng (Dueling DQN or Double Dueling DQN)\n",
        "usedoubleDQN=False\n",
        "useDueling=False\n",
        "\n",
        "UseClonedTrack=False # forces the same Track for all episodes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M7SRgxRm5Ti-"
      },
      "source": [
        "Run the new netwwork with random policy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z0YCI9TL5Szv"
      },
      "outputs": [],
      "source": [
        "# randomnet=RandomNet(n_actions)\n",
        "# createMovie(randomnet,\"random\",False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OsEv2cZLwy3e"
      },
      "source": [
        "Initialization and Parameters:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6yygleEXxEpG"
      },
      "outputs": [],
      "source": [
        "env = gym.make(\"CarRacing-v0\")\n",
        "spec = gym.spec(\"CarRacing-v0\")\n",
        "\n",
        "\n",
        "#network DQN\n",
        "policy_net = DQN(FrameSize*FrameStack, n_actions).to(device)\n",
        "target_net = DQN(FrameSize*FrameStack, n_actions).to(device)\n",
        "\n",
        "policy_net.load_state_dict(torch.load(\"/results/BestCarRacing_576_649.1408934707773_model.ckpt\"))\n",
        "\n",
        "target_net.load_state_dict(policy_net.state_dict())\n",
        "target_net.eval()\n",
        "\n",
        "buffer = ReplayBuffer(5000);  # max number of experiences in the buffer (may need to be smaller if CUDA memory errors occur).\n",
        "\n",
        "optimizer = torch.optim.Adam(policy_net.parameters(), lr=LearningRate)\n",
        "scheduler = optim.lr_scheduler.ExponentialLR(optimizer,gamma=LearningRateDecay)\n",
        "\n",
        "loss=  torch.nn.SmoothL1Loss()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VVf5wZAHxOST"
      },
      "source": [
        "DQN training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IPMrYDxyxVT1"
      },
      "outputs": [],
      "source": [
        "def trainModel(buffer_data):\n",
        "    if buffer_data.counter < BatchSize:\n",
        "        return 0.0\n",
        "\n",
        "    state_batch, action_batch, reward_batch, new_state_batch, done_batch = buffer_data.sample_buffer(BatchSize)\n",
        "    state_batch = torch.stack(state_batch).to(device)\n",
        "    action_batch = torch.stack(action_batch).reshape(-1,1).to(device)\n",
        "    new_state_batch = torch.stack(new_state_batch).to(device)\n",
        "    reward_batch = torch.stack(reward_batch).reshape(-1,1).to(device)\n",
        "    done_batch = torch.stack(done_batch).reshape(-1,1).to(device)\n",
        "\n",
        " \n",
        "    if usedoubleDQN:\n",
        "     \n",
        "        #Add Double DQN code here\n",
        "        print('Double DQN code Missing')\n",
        "\n",
        "    else: \n",
        "        q_actual = torch.gather(policy_net(state_batch),1,index=action_batch)\n",
        "        with torch.no_grad():\n",
        "            q_max_next, _ = target_net(new_state_batch).max(dim=1)\n",
        "            q_max_next=q_max_next.reshape(-1, 1)\n",
        "\n",
        "        q_target = (q_max_next * discount_factor)*(1-done_batch) + reward_batch\n",
        "        \n",
        "        ll=loss(q_actual, q_target)\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    ll.backward()\n",
        "    torch.nn.utils.clip_grad_norm_(policy_net.parameters(), 1)\n",
        "    optimizer.step()\n",
        "\n",
        "    return ll.item();"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SiLK6ZG3xlUd"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wv8KK14wxs5r"
      },
      "outputs": [],
      "source": [
        "loss_val,scores, episodes,events, avg_scores,avg_scores20,exploration = [],[],[], [],[], [], []\n",
        "\n",
        "bestScore=-99999;\n",
        "at=0;\n",
        "bestNet=copy.deepcopy(policy_net);\n",
        "\n",
        "file_name = mt.set_name(\"arch\",BatchSize,exploration_threshold,exploration_threshold_min,exploration_decay,discount_factor,LearningRate,LearningRateDecay)\n",
        "file_path_ = mt.create_dir(\"results\",file_name)\n",
        "for f in range(TotalEpisodes):\n",
        "    print(\"Episode: \"+str(f)+\" of \"+str(TotalEpisodes))\n",
        "    done  = False\n",
        "    score = 0.0\n",
        "    tloss = 0.0\n",
        " \n",
        "\n",
        "    # disable to train using random tracks\n",
        "    if(UseClonedTrack):\n",
        "       env.seed((STUDENTID1+STUDENTID2)%100);\n",
        "    else:\n",
        "       env.seed(time.time_ns());\n",
        "    \n",
        "    new_state=env.reset()\n",
        "   \n",
        " \n",
        "    stackedState=[np.rollaxis(new_state, 2, 0).copy() for i in range(FrameStack)];\n",
        "    state =torch.from_numpy(np.reshape(np.array(stackedState),(FrameSize*FrameStack,96,96)) ).float().to(device)\n",
        "   \n",
        "    if f % FreezeCounter == 0:\n",
        "       print(\"########################################\"+str(f)+\" of \"+str(TotalEpisodes))\n",
        "       target_net.load_state_dict(policy_net.state_dict())\n",
        "       \n",
        "    if f % SaveAtCounter == 0:\n",
        "      #torch.save(policy_net.state_dict(), \"v2CarRacing_\"+str(f)+'_model.ckpt')\n",
        "       createMovie(policy_net,file_path_,\"CarRacing_\"+str(f),UseClonedTrack)\n",
        "       \n",
        "    resetStatus=0;\n",
        "    i=0    \n",
        "    \n",
        "    for F in range(MaxSteps):\n",
        "        #env.render()\n",
        "        action = policy_net.getPolicy(state,exploration_threshold)\n",
        "        reward=0\n",
        "        for _ in range(ControlSteps):\n",
        "            new_state, r, done, info = env.step(action_space[action.item()])\n",
        "            reward+=r\n",
        "            i=i+1\n",
        "            if(done):\n",
        "                break\n",
        "        if reward<0 and i>InitCounter:\n",
        "            resetStatus=resetStatus+1;\n",
        "        else:\n",
        "            resetStatus=0\n",
        "            \n",
        "        stackedState.pop(0)\n",
        "        stackedState.append(np.rollaxis(new_state, 2, 0).copy())\n",
        "        new_state=torch.from_numpy(np.reshape(np.array(stackedState),(FrameSize*FrameStack,96,96))).float().to(device)\n",
        "        \n",
        "        score += reward\n",
        "        if(F<(MaxSteps-1)):\n",
        "            buffer.store_tuples(state, action, torch.tensor(reward), new_state, torch.tensor(int(done)))\n",
        "            \n",
        "        state = new_state\n",
        "        trainModel(buffer)\n",
        "\n",
        "\n",
        "        if(done or resetStatus>=ResetCounter):\n",
        "            break      \n",
        "\n",
        "        if(score < -5): break;  \n",
        "    exploration_threshold= exploration_threshold*exploration_decay if exploration_threshold > exploration_threshold_min else exploration_threshold_min\n",
        "\n",
        "    if(score>bestScore):\n",
        "        print(score,F)\n",
        "        bestScore=score;\n",
        "        bestNet=copy.deepcopy(policy_net);\n",
        "        at=f;\n",
        "    \n",
        "    exploration.append(exploration_threshold)\n",
        "    scores.append(score)\n",
        "    episodes.append(f)\n",
        "    events.append(F)\n",
        "    avg_scores.append(score/F)\n",
        "    avg_scores20.append(np.mean(scores[-20:]))\n",
        "\n",
        "    if avg_scores20[-1] > 250: break;\n",
        "\n",
        "\n",
        "torch.save(bestNet.state_dict(), \"BestCarRacing_\"+str(f)+'_'+str(bestScore)+'_model.ckpt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import importlib\n",
        "importlib.reload(eval)\n",
        "# /performance_evaluation)\n",
        "\n",
        "# from PartI_lib import performance_evaluation as eval\n",
        "eval.performance_evaluation(file_path_,episodes, scores, events, avg_scores, avg_scores20, exploration,[])\n",
        "eval.report(file_path_,\"arch\",BatchSize,exploration_threshold,exploration_threshold_min,exploration_decay,discount_factor,\n",
        "                    LearningRate,LearningRateDecay,episodes, scores, events, avg_scores, avg_scores20, exploration, 0,[])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_qEpTPbGxu_M"
      },
      "source": [
        "Plot performance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uWwFBt0vxxEd"
      },
      "outputs": [],
      "source": [
        "figure(figsize=(12, 6), dpi=80)\n",
        "plt.plot(episodes, scores)\n",
        "plt.plot(episodes, events)\n",
        "plt.plot(episodes, avg_scores)\n",
        "plt.plot(episodes, avg_scores20)\n",
        "plt.plot(episodes, exploration)\n",
        "plt.xlabel('episodes')\n",
        "plt.ylabel('y axis label')\n",
        " \n",
        "plt.title('Track Locked DQN '+str(usedoubleDQN)+' '+str(useDueling))\n",
        "plt.legend(['scores', 'events', 'avg_scores', 'avg_scores20','exploration'])\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i7IDgBVCx3mZ"
      },
      "source": [
        "View Best Results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T9RpF49oOsZj"
      },
      "outputs": [],
      "source": [
        "createMovie(bestNet,file_path_,'bestNet',UseClonedTrack)\n",
        "\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "DeepRL_PartII_updated.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
