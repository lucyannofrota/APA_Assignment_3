{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ssxiwwi66Se3"
      },
      "source": [
        "Install dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "idJX2upP6O8W"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The system cannot find the path specified.\n",
            "The system cannot find the path specified.\n",
            "'apt-get' is not recognized as an internal or external command,\n",
            "operable program or batch file.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: gym[box2d]==0.17.* in d:\\anaconda3\\envs\\apa\\lib\\site-packages (0.17.3)\n",
            "Collecting pyvirtualdisplay==0.2.*\n",
            "  Using cached PyVirtualDisplay-0.2.5-py2.py3-none-any.whl (13 kB)\n",
            "Collecting PyOpenGL==3.1.*\n",
            "  Using cached PyOpenGL-3.1.5-py3-none-any.whl (2.4 MB)\n",
            "Collecting PyOpenGL-accelerate==3.1.*\n",
            "  Using cached PyOpenGL_accelerate-3.1.5-cp38-cp38-win_amd64.whl (340 kB)\n",
            "Requirement already satisfied: numpy>=1.10.4 in d:\\anaconda3\\envs\\apa\\lib\\site-packages (from gym[box2d]==0.17.*) (1.20.3)\n",
            "Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in d:\\anaconda3\\envs\\apa\\lib\\site-packages (from gym[box2d]==0.17.*) (1.5.0)\n",
            "Requirement already satisfied: scipy in d:\\anaconda3\\envs\\apa\\lib\\site-packages (from gym[box2d]==0.17.*) (1.7.1)\n",
            "Requirement already satisfied: cloudpickle<1.7.0,>=1.2.0 in d:\\anaconda3\\envs\\apa\\lib\\site-packages (from gym[box2d]==0.17.*) (1.6.0)\n",
            "Collecting box2d-py~=2.3.5\n",
            "  Using cached box2d-py-2.3.8.tar.gz (374 kB)\n",
            "Requirement already satisfied: EasyProcess in d:\\anaconda3\\envs\\apa\\lib\\site-packages (from pyvirtualdisplay==0.2.*) (0.3)\n",
            "Requirement already satisfied: future in d:\\anaconda3\\envs\\apa\\lib\\site-packages (from pyglet<=1.5.0,>=1.4.0->gym[box2d]==0.17.*) (0.18.2)\n",
            "Building wheels for collected packages: box2d-py\n",
            "  Building wheel for box2d-py (setup.py): started\n",
            "  Building wheel for box2d-py (setup.py): finished with status 'error'\n",
            "  Running setup.py clean for box2d-py\n",
            "Failed to build box2d-py\n",
            "Installing collected packages: box2d-py, pyvirtualdisplay, PyOpenGL-accelerate, PyOpenGL\n",
            "    Running setup.py install for box2d-py: started\n",
            "    Running setup.py install for box2d-py: finished with status 'error'\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  ERROR: Command errored out with exit status 1:\n",
            "   command: 'D:\\anaconda3\\envs\\APA\\python.exe' -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '\"'\"'C:\\\\Users\\\\lucya\\\\AppData\\\\Local\\\\Temp\\\\pip-install-7dx9q5bw\\\\box2d-py_97250566fe6e424eb540acf8bd003488\\\\setup.py'\"'\"'; __file__='\"'\"'C:\\\\Users\\\\lucya\\\\AppData\\\\Local\\\\Temp\\\\pip-install-7dx9q5bw\\\\box2d-py_97250566fe6e424eb540acf8bd003488\\\\setup.py'\"'\"';f=getattr(tokenize, '\"'\"'open'\"'\"', open)(__file__);code=f.read().replace('\"'\"'\\r\\n'\"'\"', '\"'\"'\\n'\"'\"');f.close();exec(compile(code, __file__, '\"'\"'exec'\"'\"'))' bdist_wheel -d 'C:\\Users\\lucya\\AppData\\Local\\Temp\\pip-wheel-u3vjju9f'\n",
            "       cwd: C:\\Users\\lucya\\AppData\\Local\\Temp\\pip-install-7dx9q5bw\\box2d-py_97250566fe6e424eb540acf8bd003488\\\n",
            "  Complete output (16 lines):\n",
            "  Using setuptools (version 58.0.4).\n",
            "  running bdist_wheel\n",
            "  running build\n",
            "  running build_py\n",
            "  creating build\n",
            "  creating build\\lib.win-amd64-3.8\n",
            "  creating build\\lib.win-amd64-3.8\\Box2D\n",
            "  copying library\\Box2D\\Box2D.py -> build\\lib.win-amd64-3.8\\Box2D\n",
            "  copying library\\Box2D\\__init__.py -> build\\lib.win-amd64-3.8\\Box2D\n",
            "  creating build\\lib.win-amd64-3.8\\Box2D\\b2\n",
            "  copying library\\Box2D\\b2\\__init__.py -> build\\lib.win-amd64-3.8\\Box2D\\b2\n",
            "  running build_ext\n",
            "  building 'Box2D._Box2D' extension\n",
            "  swigging Box2D\\Box2D.i to Box2D\\Box2D_wrap.cpp\n",
            "  swig.exe -python -c++ -IBox2D -small -O -includeall -ignoremissing -w201 -globals b2Globals -outdir library\\Box2D -keyword -w511 -D_SWIG_KWARGS -o Box2D\\Box2D_wrap.cpp Box2D\\Box2D.i\n",
            "  error: command 'swig.exe' failed: No such file or directory\n",
            "  ----------------------------------------\n",
            "  ERROR: Failed building wheel for box2d-py\n",
            "    ERROR: Command errored out with exit status 1:\n",
            "     command: 'D:\\anaconda3\\envs\\APA\\python.exe' -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '\"'\"'C:\\\\Users\\\\lucya\\\\AppData\\\\Local\\\\Temp\\\\pip-install-7dx9q5bw\\\\box2d-py_97250566fe6e424eb540acf8bd003488\\\\setup.py'\"'\"'; __file__='\"'\"'C:\\\\Users\\\\lucya\\\\AppData\\\\Local\\\\Temp\\\\pip-install-7dx9q5bw\\\\box2d-py_97250566fe6e424eb540acf8bd003488\\\\setup.py'\"'\"';f=getattr(tokenize, '\"'\"'open'\"'\"', open)(__file__);code=f.read().replace('\"'\"'\\r\\n'\"'\"', '\"'\"'\\n'\"'\"');f.close();exec(compile(code, __file__, '\"'\"'exec'\"'\"'))' install --record 'C:\\Users\\lucya\\AppData\\Local\\Temp\\pip-record-5rfsjxyp\\install-record.txt' --single-version-externally-managed --compile --install-headers 'D:\\anaconda3\\envs\\APA\\Include\\box2d-py'\n",
            "         cwd: C:\\Users\\lucya\\AppData\\Local\\Temp\\pip-install-7dx9q5bw\\box2d-py_97250566fe6e424eb540acf8bd003488\\\n",
            "    Complete output (16 lines):\n",
            "    Using setuptools (version 58.0.4).\n",
            "    running install\n",
            "    running build\n",
            "    running build_py\n",
            "    creating build\n",
            "    creating build\\lib.win-amd64-3.8\n",
            "    creating build\\lib.win-amd64-3.8\\Box2D\n",
            "    copying library\\Box2D\\Box2D.py -> build\\lib.win-amd64-3.8\\Box2D\n",
            "    copying library\\Box2D\\__init__.py -> build\\lib.win-amd64-3.8\\Box2D\n",
            "    creating build\\lib.win-amd64-3.8\\Box2D\\b2\n",
            "    copying library\\Box2D\\b2\\__init__.py -> build\\lib.win-amd64-3.8\\Box2D\\b2\n",
            "    running build_ext\n",
            "    building 'Box2D._Box2D' extension\n",
            "    swigging Box2D\\Box2D.i to Box2D\\Box2D_wrap.cpp\n",
            "    swig.exe -python -c++ -IBox2D -small -O -includeall -ignoremissing -w201 -globals b2Globals -outdir library\\Box2D -keyword -w511 -D_SWIG_KWARGS -o Box2D\\Box2D_wrap.cpp Box2D\\Box2D.i\n",
            "    error: command 'swig.exe' failed: No such file or directory\n",
            "    ----------------------------------------\n",
            "ERROR: Command errored out with exit status 1: 'D:\\anaconda3\\envs\\APA\\python.exe' -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '\"'\"'C:\\\\Users\\\\lucya\\\\AppData\\\\Local\\\\Temp\\\\pip-install-7dx9q5bw\\\\box2d-py_97250566fe6e424eb540acf8bd003488\\\\setup.py'\"'\"'; __file__='\"'\"'C:\\\\Users\\\\lucya\\\\AppData\\\\Local\\\\Temp\\\\pip-install-7dx9q5bw\\\\box2d-py_97250566fe6e424eb540acf8bd003488\\\\setup.py'\"'\"';f=getattr(tokenize, '\"'\"'open'\"'\"', open)(__file__);code=f.read().replace('\"'\"'\\r\\n'\"'\"', '\"'\"'\\n'\"'\"');f.close();exec(compile(code, __file__, '\"'\"'exec'\"'\"'))' install --record 'C:\\Users\\lucya\\AppData\\Local\\Temp\\pip-record-5rfsjxyp\\install-record.txt' --single-version-externally-managed --compile --install-headers 'D:\\anaconda3\\envs\\APA\\Include\\box2d-py' Check the logs for full command output.\n"
          ]
        }
      ],
      "source": [
        "!pip install gym pyvirtualdisplay > /dev/null 2>&1\n",
        "!apt-get install -y xvfb python-opengl ffmpeg > /dev/null 2>&1\n",
        "# install required system dependencies\n",
        "!apt-get install -y xvfb x11-utils\n",
        "\n",
        "# install required python dependencies (might need to install additional gym extras depending)\n",
        "!pip install gym[box2d]==0.17.* pyvirtualdisplay==0.2.* PyOpenGL==3.1.* PyOpenGL-accelerate==3.1.*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "473bp5sKwJTB"
      },
      "source": [
        "Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "-1O4fl6bwGG_"
      },
      "outputs": [
        {
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'pyvirtualdisplay'",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_17936/3671673937.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mbase64\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mIPython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdisplay\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mHTML\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 21\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mpyvirtualdisplay\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mDisplay\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     22\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mIPython\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mdisplay\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mipythondisplay\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'pyvirtualdisplay'"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "from matplotlib.pyplot import figure\n",
        "\n",
        "import gym\n",
        "import torch \n",
        "from torch.utils import data\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.autograd as autograd\n",
        "import random\n",
        "from gym import wrappers\n",
        "import copy\n",
        "import gym\n",
        "from gym.wrappers import Monitor\n",
        "import glob\n",
        "import io\n",
        "import base64\n",
        "from IPython.display import HTML\n",
        "from pyvirtualdisplay import Display\n",
        "from IPython import display as ipythondisplay\n",
        "\n",
        "display = Display(visible=0, size=(1400, 900))\n",
        "display.start()\n",
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iSvUkpCgwRL9"
      },
      "source": [
        "Deep RL Replay Buffer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IIeU54J8wXkI"
      },
      "outputs": [],
      "source": [
        "\n",
        "class ReplayBuffer:\n",
        "    def __init__(self, size, input_shape,dev):\n",
        "        self.size = size\n",
        "        self.counter = 0\n",
        "        self.state_buffer = torch.zeros((self.size, input_shape))\n",
        "        self.action_buffer = torch.zeros(self.size, dtype=torch.int64)\n",
        "        self.reward_buffer = torch.zeros(self.size)\n",
        "        self.new_state_buffer = torch.zeros((self.size, input_shape))\n",
        "        self.terminal_buffer = torch.zeros(self.size)\n",
        "        self.state_buffer.to(dev)\n",
        "        self.action_buffer.to(dev)\n",
        "        self.reward_buffer.to(dev)\n",
        "        self.new_state_buffer.to(dev)\n",
        "        self.terminal_buffer.to(dev)\n",
        "        self.dev=dev;\n",
        "\n",
        "\n",
        "\n",
        "    def store_tuples(self, state, action, reward, new_state, done):\n",
        "        idx = self.counter % self.size\n",
        "        self.state_buffer[idx] = state\n",
        "        self.action_buffer[idx] = action\n",
        "        self.reward_buffer[idx] = reward\n",
        "        self.new_state_buffer[idx] = new_state\n",
        "        self.terminal_buffer[idx] = done\n",
        "        self.counter += 1\n",
        "\n",
        "    def sample_buffer(self, batch_size):\n",
        "        max_buffer = min(self.counter, self.size)\n",
        "        batch = np.random.choice(max_buffer, batch_size, replace=False)\n",
        "        state_batch = self.state_buffer[batch].to(self.dev)\n",
        "        action_batch = self.action_buffer[batch].to(self.dev)\n",
        "        reward_batch = self.reward_buffer[batch].to(self.dev)\n",
        "        new_state_batch = self.new_state_buffer[batch].to(self.dev)\n",
        "        done_batch = self.terminal_buffer[batch].to(self.dev)\n",
        "\n",
        "        return state_batch, action_batch, reward_batch, new_state_batch, done_batch\n",
        "    def last_buffer(self, batch_size):\n",
        "        state_batch = self.state_buffer[-1:-batch_size].to(self.dev)\n",
        "        action_batch = self.action_buffer[-1:-batch_size].to(self.dev)\n",
        "        reward_batch = self.reward_buffer[-1:-batch_size].to(self.dev)\n",
        "        new_state_batch = self.new_state_buffer[-1:-batch_size].to(self.dev)\n",
        "        done_batch = self.terminal_buffer[-1:-batch_size].to(self.dev)\n",
        "\n",
        "        return state_batch, action_batch, reward_batch, new_state_batch, done_batch\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fG0T040FwczS"
      },
      "source": [
        "Default DQN arquitecture"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aGrl7mfpwgfM"
      },
      "outputs": [],
      "source": [
        "class DQN(nn.Module):\n",
        "\n",
        "    def __init__(self, inputs, outputs,dfactor):\n",
        "        super(DQN, self).__init__()\n",
        "        \n",
        "        self.input_size=inputs;\n",
        "        self.output_size=outputs;\n",
        "        self.discount_factor=dfactor;\n",
        "        \n",
        "        self.layers = nn.Sequential(\n",
        "           #Add your code here\n",
        "        )\n",
        "\n",
        "\n",
        "    # Called with either one element to determine next action, or a batch\n",
        "    # during optimization. Returns tensor([[left0exp,right0exp]...]).\n",
        "    def forward(self, x):\n",
        "        return self.layers(x)\n",
        "\n",
        "    def policy(self,state):\n",
        "       with torch.no_grad():\n",
        "            return self.__call__(state).argmax()\n",
        "     \n",
        "    def getPolicy(self,state,eps_threshold):\n",
        "        sample = random.random()\n",
        "        if sample > eps_threshold:\n",
        "            with torch.no_grad():\n",
        " \n",
        "                return self.__call__(state).argmax()\n",
        "        else:\n",
        "            return  torch.tensor([[random.randrange(self.output_size)]], device=device, dtype=torch.long)\n",
        "\n",
        "\n",
        "\n",
        " "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e9Cz-vtcwv1s"
      },
      "source": [
        "OpenAI Environment CartPole-v0\n",
        "\n",
        "###    Description:\n",
        "        A pole is attached by an un-actuated joint to a cart, which moves along a frictionless track. The pendulum starts upright, and the goal is to prevent it from falling over by increasing and reducing the cart's velocity.\n",
        "###    Source:\n",
        "        This environment corresponds to the version of the cart-pole problem described by Barto, Sutton, and Anderson\n",
        "###    Observation:\n",
        "        Type: Box(4)\n",
        "        Num     Observation               Min                     Max\n",
        "        0       Cart Position             -4.8                    4.8\n",
        "        1       Cart Velocity             -Inf                    Inf\n",
        "        2       Pole Angle                -0.418 rad (-24 deg)    0.418 rad (24 deg)\n",
        "        3       Pole Angular Velocity     -Inf                    Inf\n",
        "###    Actions:\n",
        "        Type: Discrete(2)\n",
        "        Num   Action\n",
        "        0     Push cart to the left\n",
        "        1     Push cart to the right\n",
        "        Note: The amount the velocity that is reduced or increased is not fixed; it depends on the angle the pole is pointing. This is because the center of gravity of the pole increases the amount of energy needed to move the cart underneath it\n",
        "###    Reward:\n",
        "        Reward is 1 for every step taken, including the termination step\n",
        "###    Starting State:\n",
        "        All observations are assigned a uniform random value in [-0.05..0.05]\n",
        "###    Episode Termination:\n",
        "        Pole Angle is more than 12 degrees.\n",
        "        Cart Position is more than 2.4 (center of the cart reaches the edge of the display).\n",
        "        Episode length is greater than 200.\n",
        "###     Solved Requirements:\n",
        "        Considered solved when the average return is greater than or equal to 195.0 over 100 consecutive trials."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "og-Pb_QrxWf_"
      },
      "source": [
        "DQN movie generation (for visual evaluation in Google Colab)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dVwFwJMNxoCw"
      },
      "outputs": [],
      "source": [
        "def createMovie(Network,Filename):\n",
        "    env = gym.make(\"CartPole-v0\")\n",
        "    envX = wrappers.Monitor(env, './videos/'+Filename,force=True)\n",
        "\n",
        "    observation = envX.reset()\n",
        "\n",
        "    i=0\n",
        "    Network.eval()\n",
        "\n",
        "    while True:\n",
        "        envX.render()\n",
        "        \n",
        "        state=torch.Tensor(observation).to(device)\n",
        "      \n",
        "        action = Network.policy(state);\n",
        "             \n",
        "        observation, reward, done, info = envX.step(action.item()) \n",
        "        i=i+1;     \n",
        "        if done: \n",
        "          break;\n",
        "\n",
        "    envX.close()\n",
        "    env.close();\n",
        "    Network.train()\n",
        "    mp4list = glob.glob('./videos/'+Filename+'/*.mp4')\n",
        "    if len(mp4list) > 0:\n",
        "        mp4 = mp4list[0]\n",
        "        video = io.open(mp4, 'r+b').read()\n",
        "        encoded = base64.b64encode(video)\n",
        "        ipythondisplay.display(HTML(data='''<video alt=\"test\" autoplay \n",
        "                loop controls style=\"height: 400px;\">\n",
        "                <source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\" />\n",
        "             </video>'''.format(encoded.decode('ascii'))))\n",
        "    else: \n",
        "        print(\"Could not find video\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LE6SfnBL4ZOh"
      },
      "source": [
        "Dummy Net with random policy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pTsrsjcExD1m"
      },
      "outputs": [],
      "source": [
        "  class RandomNet(nn.Module):\n",
        "\n",
        "    def __init__(self,outputs):\n",
        "        super(RandomNet, self).__init__()        \n",
        "        self.output_size=outputs;\n",
        "        \n",
        " \n",
        "    def forward(self, x):\n",
        "        return  x\n",
        "\n",
        "    def policy(self,state):\n",
        "       return  torch.tensor([[random.randrange(self.output_size)]], device=device, dtype=torch.long)\n",
        "     \n",
        "    \n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M7SRgxRm5Ti-"
      },
      "source": [
        "Run the new netwwork with random policy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z0YCI9TL5Szv"
      },
      "outputs": [],
      "source": [
        "randomnet=RandomNet(2)\n",
        "createMovie(randomnet,\"random\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OsEv2cZLwy3e"
      },
      "source": [
        "Initialization and Parameters:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6yygleEXxEpG"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "# Sim configuration\n",
        "env = gym.make(\"CartPole-v0\")\n",
        "spec = gym.spec(\"CartPole-v0\")\n",
        "\n",
        "\n",
        "\n",
        "inputs=4\n",
        "n_actions=2\n",
        "\n",
        "#hyper-parameters\n",
        "TotalEpisodes=1000;\n",
        "MaxSteps=200;\n",
        "FreezeCounter=25;\n",
        "BatchSize=128;\n",
        "exploration_threshold=1\n",
        "exploration_threshold_min=0.01\n",
        "exploration_decay=0.002\n",
        "discount_factor=0.99\n",
        "SaveAtCounter=25\n",
        "LearningRate=0.001\n",
        "\n",
        "\n",
        "#network DQN\n",
        "policy_net = DQN(inputs, n_actions,discount_factor).to(device)\n",
        "target_net = DQN(inputs, n_actions,discount_factor).to(device)\n",
        "target_net.load_state_dict(policy_net.state_dict())\n",
        "target_net.eval()\n",
        "\n",
        "buffer = ReplayBuffer(1000000, inputs,device);\n",
        "\n",
        "optimizer = torch.optim.Adam(policy_net.parameters(), lr=LearningRate)\n",
        "loss = torch.nn.MSELoss()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VVf5wZAHxOST"
      },
      "source": [
        "DQN training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IPMrYDxyxVT1"
      },
      "outputs": [],
      "source": [
        "def trainModel():\n",
        "    if buffer.counter < BatchSize:\n",
        "        return 0.0\n",
        "         \n",
        "    state_batch, action_batch, reward_batch, new_state_batch, done_batch = buffer.sample_buffer(BatchSize)\n",
        "\n",
        "    q_actual = torch.gather(policy_net(state_batch),1,action_batch.reshape(-1,1))\n",
        "    with torch.no_grad():\n",
        "        q_max_next = target_net(new_state_batch).max(1)[0].detach()\n",
        "    q_target = (q_max_next * discount_factor)*(1-done_batch) + reward_batch\n",
        "\n",
        "    ll=loss(q_actual, q_target.unsqueeze(1))\n",
        "\n",
        "    # Optimize the model\n",
        "    optimizer.zero_grad()\n",
        "    ll.backward()\n",
        "    for param in policy_net.parameters():\n",
        "        param.grad.data.clamp_(-1, 1)\n",
        "    optimizer.step()\n",
        "\n",
        "    return ll.item();"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SiLK6ZG3xlUd"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wv8KK14wxs5r"
      },
      "outputs": [],
      "source": [
        "loss_val,scores, episodes,events, avg_scores,avg_scores100,exploration = [],[],[], [],[], [], []\n",
        "\n",
        "bestScore=-99999;\n",
        "bestNet=copy.deepcopy(policy_net);\n",
        "fx=0\n",
        "for f in range(TotalEpisodes):\n",
        "    done  = False\n",
        "    score = 0.0\n",
        "    state = torch.Tensor(env.reset()).to(device)\n",
        "    if f % FreezeCounter == 0:\n",
        "       print(str(f)+\" of \"+str(TotalEpisodes))\n",
        "       target_net.load_state_dict(policy_net.state_dict())\n",
        "\n",
        "    if f % SaveAtCounter == 0:\n",
        "       torch.save(policy_net.state_dict(), \"v2CartPole_\"+str(f)+'_model.ckpt')\n",
        "       createMovie(policy_net,\"v2CartPole_\"+str(f))\n",
        "\n",
        "    for F in range(MaxSteps):\n",
        "        action = policy_net.getPolicy(state,exploration_threshold)\n",
        "        new_state, reward, done, _ = env.step(action.item())\n",
        "\n",
        "        new_state=torch.Tensor(new_state).to(device);\n",
        "        score += reward\n",
        "        if(F<(MaxSteps-1)):  # avoid adding the last \"good\" example as done\n",
        "            buffer.store_tuples(state, action, reward, new_state, done)\n",
        "        state = new_state\n",
        "        trainModel()\n",
        "        if(done):\n",
        "            break        \n",
        "    exploration_threshold= exploration_threshold-exploration_decay if exploration_threshold > exploration_threshold_min else exploration_threshold_min\n",
        "\n",
        "    \n",
        "    #log results\n",
        "    exploration.append(exploration_threshold)\n",
        "    scores.append(score)\n",
        "    episodes.append(f)\n",
        "    events.append(F)\n",
        "    avg_scores.append(score/F)\n",
        "    avg_scores100.append(np.mean(scores[-100:]))\n",
        "\n",
        "    if(score>=bestScore):\n",
        "        print(score,F+1,np.mean(scores[-100:]))\n",
        "        bestScore=score;\n",
        "        fx=f;\n",
        "        bestNet=copy.deepcopy(policy_net);\n",
        "\n",
        "torch.save(bestNet.state_dict(), \"BestCartPole\"+str(fx)+'_'+str(bestScore)+'_model.ckpt')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_qEpTPbGxu_M"
      },
      "source": [
        "Plot performance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uWwFBt0vxxEd"
      },
      "outputs": [],
      "source": [
        "figure(figsize=(12, 6), dpi=80)\n",
        "plt.plot(episodes, scores)\n",
        "plt.plot(episodes, events)\n",
        "plt.plot(episodes, avg_scores)\n",
        "plt.plot(episodes, avg_scores100)\n",
        "plt.plot(episodes, exploration)\n",
        "plt.xlabel('episodes')\n",
        "plt.ylabel('y axis label')\n",
        "plt.title('Report')\n",
        "plt.legend(['scores',  'events', 'avg_scores', 'avg_scores100','exploration'])\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i7IDgBVCx3mZ"
      },
      "source": [
        "View Best Results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T9RpF49oOsZj"
      },
      "outputs": [],
      "source": [
        "createMovie(bestNet,'bestNet')"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "DeepRL_PartI.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
