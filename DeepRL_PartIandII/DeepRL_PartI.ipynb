{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ssxiwwi66Se3"
      },
      "source": [
        "Install dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "idJX2upP6O8W",
        "outputId": "af0cfcf7-ac74-4fa7-bf34-265362c01bba"
      },
      "outputs": [],
      "source": [
        "!pip install gym pyvirtualdisplay > /dev/null 2>&1\n",
        "!apt-get install -y xvfb python-opengl ffmpeg > /dev/null 2>&1\n",
        "# install required system dependencies\n",
        "!apt-get install -y xvfb x11-utils\n",
        "\n",
        "# install required python dependencies (might need to install additional gym extras depending)\n",
        "!pip install gym[box2d]==0.17.* pyvirtualdisplay==0.2.* PyOpenGL==3.1.* PyOpenGL-accelerate==3.1.*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-1O4fl6bwGG_",
        "outputId": "b555e1ae-2aea-4089-e052-d4ecf0483257"
      },
      "outputs": [],
      "source": [
        "#@title Imports\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "from matplotlib.pyplot import figure\n",
        "\n",
        "import gym\n",
        "import torch\n",
        "from torch.utils import data\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.autograd as autograd\n",
        "import random\n",
        "from gym import wrappers\n",
        "import copy\n",
        "import gym\n",
        "from gym.wrappers import Monitor\n",
        "import glob\n",
        "import io\n",
        "import base64\n",
        "from IPython.display import HTML\n",
        "from pyvirtualdisplay import Display\n",
        "from IPython import display as ipythondisplay\n",
        "\n",
        "display = Display(visible=0, size=(1400, 900))\n",
        "display.start()\n",
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "print(torch.cuda.is_available())\n",
        "print(device)\n",
        "\n",
        "from PartI_lib import archs as archs\n",
        "from PartI_lib import performance_evaluation as eval\n",
        "from PartI_lib import train_loop as tl\n",
        "from PartI_lib import my_tools as mt\n",
        "# !mkdir Checkpoints\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IIeU54J8wXkI"
      },
      "outputs": [],
      "source": [
        "#@title Deep RL Replay Buffer\n",
        "\n",
        "class ReplayBuffer:\n",
        "    def __init__(self, size, input_shape,dev):\n",
        "        self.size = size\n",
        "        self.counter = 0\n",
        "        self.state_buffer = torch.zeros((self.size, input_shape))\n",
        "        self.action_buffer = torch.zeros(self.size, dtype=torch.int64)\n",
        "        self.reward_buffer = torch.zeros(self.size)\n",
        "        self.new_state_buffer = torch.zeros((self.size, input_shape))\n",
        "        self.terminal_buffer = torch.zeros(self.size)\n",
        "        self.state_buffer.to(dev)\n",
        "        self.action_buffer.to(dev)\n",
        "        self.reward_buffer.to(dev)\n",
        "        self.new_state_buffer.to(dev)\n",
        "        self.terminal_buffer.to(dev)\n",
        "        self.dev=dev;\n",
        "\n",
        "\n",
        "\n",
        "    def store_tuples(self, state, action, reward, new_state, done):\n",
        "        idx = self.counter % self.size\n",
        "        self.state_buffer[idx] = state\n",
        "        self.action_buffer[idx] = action\n",
        "        self.reward_buffer[idx] = reward\n",
        "        self.new_state_buffer[idx] = new_state\n",
        "        self.terminal_buffer[idx] = done\n",
        "        self.counter += 1\n",
        "\n",
        "    def sample_buffer(self, batch_size):\n",
        "        max_buffer = min(self.counter, self.size)\n",
        "        batch = np.random.choice(max_buffer, batch_size, replace=False)\n",
        "        state_batch = self.state_buffer[batch].to(self.dev)\n",
        "        action_batch = self.action_buffer[batch].to(self.dev)\n",
        "        reward_batch = self.reward_buffer[batch].to(self.dev)\n",
        "        new_state_batch = self.new_state_buffer[batch].to(self.dev)\n",
        "        done_batch = self.terminal_buffer[batch].to(self.dev)\n",
        "\n",
        "        return state_batch, action_batch, reward_batch, new_state_batch, done_batch\n",
        "    def last_buffer(self, batch_size):\n",
        "        state_batch = self.state_buffer[-1:-batch_size].to(self.dev)\n",
        "        action_batch = self.action_buffer[-1:-batch_size].to(self.dev)\n",
        "        reward_batch = self.reward_buffer[-1:-batch_size].to(self.dev)\n",
        "        new_state_batch = self.new_state_buffer[-1:-batch_size].to(self.dev)\n",
        "        done_batch = self.terminal_buffer[-1:-batch_size].to(self.dev)\n",
        "\n",
        "        return state_batch, action_batch, reward_batch, new_state_batch, done_batch\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fG0T040FwczS"
      },
      "source": [
        "Default DQN arquitecture"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aGrl7mfpwgfM"
      },
      "outputs": [],
      "source": [
        "# class DQN(nn.Module):\n",
        "\n",
        "#     def __init__(self, inputs, outputs,dfactor):\n",
        "#         super(DQN, self).__init__()\n",
        "        \n",
        "#         self.input_size=inputs;\n",
        "#         self.output_size=outputs;\n",
        "#         self.discount_factor=dfactor;\n",
        "        \n",
        "#         self.layers = nn.Sequential(\n",
        "#             nn.Linear(in_features=self.input_size, out_features=128),\n",
        "#             # nn.Linear(in_features=128, out_features=256),\n",
        "#             # nn.Linear(in_features=256, out_features=512),\n",
        "#             # nn.Linear(in_features=512, out_features=256),\n",
        "#             nn.Linear(in_features=128, out_features=self.output_size)\n",
        "#         )\n",
        "\n",
        "\n",
        "#     # Called with either one element to determine next action, or a batch\n",
        "#     # during optimization. Returns tensor([[left0exp,right0exp]...]).\n",
        "#     def forward(self, x):\n",
        "#         return self.layers(x)\n",
        "\n",
        "#     def policy(self,state):\n",
        "#        with torch.no_grad():\n",
        "#             return self.__call__(state).argmax()\n",
        "     \n",
        "#     def getPolicy(self,state,eps_threshold):\n",
        "#         sample = random.random()\n",
        "#         if sample > eps_threshold:\n",
        "#             with torch.no_grad():\n",
        " \n",
        "#                 return self.__call__(state).argmax()\n",
        "#         else:\n",
        "#             return  torch.tensor([[random.randrange(self.output_size)]], device=device, dtype=torch.long)\n",
        "\n",
        "\n",
        "\n",
        " "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e9Cz-vtcwv1s"
      },
      "source": [
        "OpenAI Environment CartPole-v0\n",
        "\n",
        "###    Description:\n",
        "        A pole is attached by an un-actuated joint to a cart, which moves along a frictionless track. The pendulum starts upright, and the goal is to prevent it from falling over by increasing and reducing the cart's velocity.\n",
        "###    Source:\n",
        "        This environment corresponds to the version of the cart-pole problem described by Barto, Sutton, and Anderson\n",
        "###    Observation:\n",
        "        Type: Box(4)\n",
        "        Num     Observation               Min                     Max\n",
        "        0       Cart Position             -4.8                    4.8\n",
        "        1       Cart Velocity             -Inf                    Inf\n",
        "        2       Pole Angle                -0.418 rad (-24 deg)    0.418 rad (24 deg)\n",
        "        3       Pole Angular Velocity     -Inf                    Inf\n",
        "###    Actions:\n",
        "        Type: Discrete(2)\n",
        "        Num   Action\n",
        "        0     Push cart to the left\n",
        "        1     Push cart to the right\n",
        "        Note: The amount the velocity that is reduced or increased is not fixed; it depends on the angle the pole is pointing. This is because the center of gravity of the pole increases the amount of energy needed to move the cart underneath it\n",
        "###    Reward:\n",
        "        Reward is 1 for every step taken, including the termination step\n",
        "###    Starting State:\n",
        "        All observations are assigned a uniform random value in [-0.05..0.05]\n",
        "###    Episode Termination:\n",
        "        Pole Angle is more than 12 degrees.\n",
        "        Cart Position is more than 2.4 (center of the cart reaches the edge of the display).\n",
        "        Episode length is greater than 200.\n",
        "###     Solved Requirements:\n",
        "        Considered solved when the average return is greater than or equal to 195.0 over 20 consecutive trials."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "dVwFwJMNxoCw"
      },
      "outputs": [],
      "source": [
        "#@title DQN movie generation (for visual evaluation in Google Colab)\n",
        "\n",
        "def createMovie(Network, path, Filename):\n",
        "    env = gym.make(\"CartPole-v0\")\n",
        "    envX = wrappers.Monitor(env, path+'/'+Filename, force=True)\n",
        "\n",
        "    observation = envX.reset()\n",
        "\n",
        "    i = 0\n",
        "    Network.eval()\n",
        "\n",
        "    while True:\n",
        "        envX.render()\n",
        "\n",
        "        state = torch.Tensor(observation).to(device)\n",
        "\n",
        "        action = Network.policy(state)\n",
        "\n",
        "        observation, reward, done, info = envX.step(action.item())\n",
        "        i = i+1\n",
        "        if done:\n",
        "          break\n",
        "\n",
        "    envX.close()\n",
        "    env.close()\n",
        "    Network.train()\n",
        "    mp4list = glob.glob(path+Filename+'/*.mp4')\n",
        "    if len(mp4list) > 0:\n",
        "        mp4 = mp4list[0]\n",
        "        video = io.open(mp4, 'r+b').read()\n",
        "        encoded = base64.b64encode(video)\n",
        "        if(Running_in_colab):\n",
        "            ipythondisplay.display(HTML(data='''<video alt=\"test\" autoplay \n",
        "                    loop controls style=\"height: 400px;\">\n",
        "                    <source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\" />\n",
        "                </video>'''.format(encoded.decode('ascii'))))\n",
        "        else:\n",
        "            # print(\"Loc Video\")\n",
        "            # print(mp4)\n",
        "            # print(\"VIDEO PR\")\n",
        "            Video(mp4)\n",
        "\n",
        "            # HTML(\"\"\"\n",
        "            #         <video alt=\"test\" controls>\n",
        "            #             <source src=\"test.mp4\" type=\"video/mp4\">\n",
        "            #         </video>\n",
        "            #     \"\"\")\n",
        "    else:\n",
        "        print(\"Could not find video\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M7SRgxRm5Ti-"
      },
      "source": [
        "Run the new netwwork with random policy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 421
        },
        "id": "Z0YCI9TL5Szv",
        "outputId": "7c9f00b2-80d3-4981-a5a9-fcd8e0c05d7a"
      },
      "outputs": [],
      "source": [
        "# randomnet=RandomNet(2)\n",
        "# createMovie(randomnet,\"random\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OsEv2cZLwy3e"
      },
      "source": [
        "Initialization and Parameters:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6yygleEXxEpG",
        "outputId": "6252a170-0e96-443e-f1eb-5fb7d078a71e"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "# Sim configuration\n",
        "env = gym.make(\"CartPole-v0\")\n",
        "spec = gym.spec(\"CartPole-v0\")\n",
        "\n",
        "\n",
        "inputs = 4\n",
        "n_actions = 2\n",
        "\n",
        "#hyper-parameters\n",
        "TotalEpisodes = 2000\n",
        "MaxSteps = 400\n",
        "FreezeCounter = 25\n",
        "BatchSize = 128\n",
        "exploration_threshold = 1\n",
        "exploration_threshold_min = 0.01\n",
        "# exploration_decay=0.002\n",
        "discount_factor = 0.99\n",
        "SaveAtCounter = 200\n",
        "LearningRateDecay = 0.99\n",
        "\n",
        "arch = \"DoubleDuelingDQN\"\n",
        "n_layers = 4\n",
        "LearningRate = 0.0001\n",
        "exploration_decay = 0.02\n",
        "\n",
        "\n",
        "# arch = \"DQN\"\n",
        "\n",
        "#network DQN\n",
        "\n",
        "buffer = ReplayBuffer(1000000, inputs, device)\n",
        "\n",
        "# print(policy_net)\n",
        "\n",
        "file_path__ = mt.create_dir(\"results\", arch)\n",
        "\n",
        "policy_net, target_net = archs.archs(\n",
        "    arch, inputs, n_actions, discount_factor, device, n_layers)\n",
        "target_net.load_state_dict(policy_net.state_dict())\n",
        "target_net.eval()\n",
        "file_path_ = mt.create_dir(file_path__, \"N\"+str(n_layers)+\"_Layers\")\n",
        "\n",
        "optimizer = torch.optim.Adam(\n",
        "    policy_net.parameters(), lr=LearningRate)\n",
        "scheduler = optim.lr_scheduler.ExponentialLR(\n",
        "    optimizer, gamma=LearningRateDecay)\n",
        "loss = torch.nn.MSELoss()\n",
        "\n",
        "def trainModel():\n",
        "    if buffer.counter < BatchSize:\n",
        "        return 0.0\n",
        "\n",
        "    state_batch, action_batch, reward_batch, new_state_batch, done_batch = buffer.sample_buffer(\n",
        "        BatchSize)\n",
        "\n",
        "    q_actual = torch.gather(policy_net(\n",
        "        state_batch), 1, action_batch.reshape(-1, 1))\n",
        "    with torch.no_grad():\n",
        "        if(arch == \"DoubleDQN\" or arch == \"DoubleDuelingDQN\"):\n",
        "            target = torch.argmax(\n",
        "                policy_net(state_batch), -1).detach()\n",
        "            q_max_next = target_net(new_state_batch).gather(\n",
        "                1, target.unsqueeze(-1)).squeeze(-1)\n",
        "        else:\n",
        "            q_max_next = target_net(new_state_batch).max(1)[\n",
        "                0].detach()\n",
        "    q_target = (q_max_next * discount_factor) * \\\n",
        "        (1-done_batch) + reward_batch\n",
        "\n",
        "    ll = loss(q_actual, q_target.unsqueeze(1))\n",
        "\n",
        "    # Optimize the model\n",
        "    optimizer.zero_grad()\n",
        "    ll.backward()\n",
        "    for param in policy_net.parameters():\n",
        "        param.grad.data.clamp_(-1, 1)\n",
        "    optimizer.step()\n",
        "\n",
        "    return ll.item()\n",
        "\n",
        "\n",
        "file_name = mt.set_name(arch, n_layers, BatchSize, exploration_threshold, exploration_threshold_min,\n",
        "                        exploration_decay, discount_factor, LearningRate, LearningRateDecay)\n",
        "file_path = mt.create_dir(file_path_, file_name)\n",
        "mt.create_dir(file_path, \"Checkpoints\")\n",
        "print(file_name)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "bestNet, episodes, scores, events, avg_scores, avg_scores20, exploration, avg_scores100 = tl.train_loop(\n",
        "    policy_net, target_net, env, device, TotalEpisodes, FreezeCounter, SaveAtCounter, createMovie, MaxSteps, exploration_threshold, exploration_decay, exploration_threshold_min, buffer, trainModel, file_path)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "eval.performance_evaluation(\n",
        "    file_path, episodes, scores, events, avg_scores, avg_scores20, exploration, avg_scores100)\n",
        "eval.report(file_path, arch, BatchSize, exploration_threshold, exploration_threshold_min, exploration_decay, discount_factor,\n",
        "            LearningRate, LearningRateDecay, episodes, scores, events, avg_scores, avg_scores20, exploration, n_layers, avg_scores100)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "createMovie(bestNet, file_path, 'bestNet')\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "DeepRL_PartI.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
