{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DeepRL_PartII.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ssxiwwi66Se3"
      },
      "source": [
        "Install dependencies"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "idJX2upP6O8W"
      },
      "source": [
        "!pip install gym pyvirtualdisplay > /dev/null 2>&1\n",
        "!apt-get install -y xvfb python-opengl ffmpeg > /dev/null 2>&1\n",
        "# install required system dependencies\n",
        "!apt-get install -y xvfb x11-utils\n",
        "\n",
        "# install required python dependencies (might need to install additional gym extras depending)\n",
        "!pip install gym[box2d]==0.17.* pyvirtualdisplay==0.2.* PyOpenGL==3.1.* PyOpenGL-accelerate==3.1.*\n",
        "\n",
        "!git clone https://github.com/openai/gym.git\n",
        "%cd gym\n",
        "!pip install -e .\n",
        "!pip install stable-baselines[mpi]\n",
        "!pip install stable-baselines3[extra]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "473bp5sKwJTB"
      },
      "source": [
        "Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-1O4fl6bwGG_"
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "from matplotlib.pyplot import figure\n",
        "\n",
        "import gym\n",
        "import torch \n",
        "from torch.utils import data\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.autograd as autograd\n",
        "import random\n",
        "from gym import wrappers\n",
        "import copy\n",
        "import gym\n",
        "from gym.wrappers import Monitor\n",
        "import glob\n",
        "import io\n",
        "import base64\n",
        "from IPython.display import HTML\n",
        "from pyvirtualdisplay import Display\n",
        "from IPython import display as ipythondisplay\n",
        "\n",
        "display = Display(visible=0, size=(1400, 900))\n",
        "display.start()\n",
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iSvUkpCgwRL9"
      },
      "source": [
        "Deep RL Replay Buffer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IIeU54J8wXkI"
      },
      "source": [
        "class ReplayBuffer:\n",
        "    def __init__(self, size):\n",
        "        self.size = size\n",
        "        self.counter = 0\n",
        "        self.state_buffer = []\n",
        "        self.action_buffer = []\n",
        "        self.reward_buffer = []\n",
        "        self.new_state_buffer = []\n",
        "        self.terminal_buffer = []\n",
        "        \n",
        "    def store_tuples(self, state, action, reward, new_state, done):\n",
        "        if len(self.state_buffer) < self.size:\n",
        "            self.state_buffer.append(None)\n",
        "            self.action_buffer.append(None)\n",
        "            self.reward_buffer.append(None)\n",
        "            self.new_state_buffer.append(None)\n",
        "            self.terminal_buffer.append(None)\n",
        "            \n",
        "        self.counter = self.counter % self.size\n",
        "        self.state_buffer[self.counter] = state\n",
        "        self.action_buffer[self.counter] = action\n",
        "        self.reward_buffer[self.counter] = reward\n",
        "        self.new_state_buffer[self.counter] = new_state\n",
        "        self.terminal_buffer[self.counter] = done\n",
        "        self.counter += 1\n",
        "\n",
        "    def sample_buffer(self, batch_size):\n",
        "        max_buffer = min(self.counter, self.size)\n",
        "        batch = np.random.choice(max_buffer, batch_size, replace=False)\n",
        "        state_batch=[];\n",
        "        action_batch=[];\n",
        "        reward_batch=[];\n",
        "        new_state_batch=[];\n",
        "        done_batch=[];\n",
        "        \n",
        "        for a in batch:\n",
        "            state_batch.append(self.state_buffer[a])\n",
        "            action_batch.append(self.action_buffer[a])\n",
        "            reward_batch.append(self.reward_buffer[a])\n",
        "            new_state_batch.append(self.new_state_buffer[a])\n",
        "            done_batch.append(self.terminal_buffer[a])\n",
        "        return state_batch, action_batch, reward_batch, new_state_batch, done_batch\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fG0T040FwczS"
      },
      "source": [
        "\n",
        "\n",
        "Default DQN arquitecture"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aGrl7mfpwgfM"
      },
      "source": [
        "class DQN(nn.Module):\n",
        "\n",
        "    def __init__(self, inputs, outputs):\n",
        "        super(DQN, self).__init__()\n",
        "        \n",
        "        self.input_size=inputs;\n",
        "        self.output_size=outputs;\n",
        "\n",
        "        \n",
        "        \n",
        "        self.features = nn.Sequential(\n",
        "            #Add code here for DQN \n",
        "\n",
        "        )\n",
        "        \n",
        "        self.layers = nn.Sequential(\n",
        "            #Add code here for DQN \n",
        "        )\n",
        "        \n",
        "       \n",
        "        self.feature = nn.Sequential(\n",
        "            #Add code here for Dueling \n",
        "        )\n",
        "\n",
        "\n",
        "        self.advantage = nn.Sequential(\n",
        "            #Add code here for Dueling \n",
        "        )\n",
        "        \n",
        "        self.value = nn.Sequential(\n",
        "           #Add code here for Dueling \n",
        "        )\n",
        "        \n",
        "\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "    \n",
        "        if(useDueling):\n",
        "            x=self.features(x)\n",
        "            x = x.view(x.size(0), -1)\n",
        "            x = self.feature(x)\n",
        "            advantage = self.advantage(x)\n",
        "            value     = self.value(x)\n",
        "            return value + advantage  - advantage.mean()\n",
        "        else:\n",
        "            x=self.features(x)\n",
        "            x = x.view(x.size(0), -1)\n",
        "            return self.layers(x)\n",
        "\n",
        "    def policy(self,state):\n",
        "       with torch.no_grad():\n",
        "            return self.__call__(state.unsqueeze(0)).argmax()\n",
        "     \n",
        "    def getPolicy(self,state,eps_threshold):\n",
        "        sample = random.random()\n",
        "        if sample > eps_threshold:\n",
        "            with torch.no_grad():\n",
        "                return self.__call__(state.unsqueeze(0)).argmax()\n",
        "        else:\n",
        "            return  torch.tensor(random.randrange(self.output_size), device=device, dtype=torch.long)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e9Cz-vtcwv1s"
      },
      "source": [
        "OpenAI Environment CarRacing-v0\n",
        "\n",
        " \n",
        "###    Description:\n",
        "        Easiest continuous control task to learn from pixels, a top-down racing environment.\n",
        "###    Observation:\n",
        "        Type: Image (96x96x3)\n",
        "        Some indicators are shown at the bottom of the window along with the state RGB buffer. From left to right: the true speed, four ABS sensors, the steering wheel position and gyroscope.\n",
        "###    Actions:\n",
        "        Discrete control is reasonable in this environment as well, on/off discretization is fine.\n",
        "###    Reward:\n",
        "        The reward is -0.1 every frame and +1000/N for every track tile visited, where N is the total number of tiles visited in the track. For example, if you have finished in 732 frames, your reward is 1000 - 0.1*732 = 926.8 points.\n",
        "###    Starting State:\n",
        "        The tracks are randomly generated (switch to True the UseClonedTrack flag to activate this behavior) for each episode.\n",
        "###    Episode Termination:\n",
        "        Car is too faraway from the track.\n",
        "        Car accumulated a negative reward after given iterations (see ResetCounter and InitCounter).\n",
        "        Episode length is greater than 1000.\n",
        "        All tiles are visited.\n",
        "###     Solved Requirements:\n",
        "        The game is solved when the agent consistently gets 900+ points.  \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "og-Pb_QrxWf_"
      },
      "source": [
        "DQN movie generation (for visual evaluation in Google Colab)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dVwFwJMNxoCw"
      },
      "source": [
        "def createMovie(Network,Filename,stateInit,trackcopy,UseClone):\n",
        "  \n",
        "    env1 = gym.make(\"CarRacing-v0\")\n",
        "    envX = wrappers.Monitor(env1, './videos/'+Filename,force=True)\n",
        "  \n",
        "    new_state = envX.reset()\n",
        "    \n",
        "    if UseClone:\n",
        "        new_state=stateInit\n",
        "        envX.track=copy.deepcopy(trackcopy);\n",
        "\n",
        "\n",
        "    \n",
        "    stackedStateX=[np.rollaxis(new_state, 2, 0).copy() for i in range(FrameStack)];\n",
        "    state =torch.from_numpy(np.reshape(np.array(stackedStateX),(FrameSize*FrameStack,96,96)) ).float().to(device)\n",
        "\n",
        "\n",
        "    i=0\n",
        "    Network.eval()\n",
        "    resetStatus=0;   \n",
        "\n",
        "    while True:\n",
        "        envX.render()\n",
        "        \n",
        "        action = Network.policy(state);\n",
        "        reward=0\n",
        "        for x in range(ControlSteps):\n",
        "            new_state, r, done, _ = envX.step(action_space[action.item()])\n",
        "            reward+=r\n",
        "            if(done):\n",
        "                break\n",
        "        if reward<0 and i>InitCounter:\n",
        "            resetStatus=resetStatus+1;\n",
        "        else:\n",
        "            resetStatus=0\n",
        "        \n",
        "        stackedStateX.pop(0)\n",
        "        \n",
        " \n",
        "        stackedStateX.append(np.rollaxis(new_state, 2, 0).copy())\n",
        "        state=torch.from_numpy(np.reshape(np.array(stackedStateX),(FrameSize*FrameStack,96,96))).float().to(device)\n",
        "        i=i+1;     \n",
        "        if done or resetStatus>=ResetCounter: \n",
        "          break;\n",
        "\n",
        "    envX.close()\n",
        "    env1.close();\n",
        "    Network.train()\n",
        "    mp4list = glob.glob('./videos/'+Filename+'/*.mp4')\n",
        "    if len(mp4list) > 0:\n",
        "        mp4 = mp4list[0]\n",
        "        video = io.open(mp4, 'r+b').read()\n",
        "        encoded = base64.b64encode(video)\n",
        "        ipythondisplay.display(HTML(data='''<video alt=\"test\" autoplay \n",
        "                loop controls style=\"height: 400px;\">\n",
        "                <source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\" />\n",
        "             </video>'''.format(encoded.decode('ascii'))))\n",
        "    else: \n",
        "        print(\"Could not find video\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LE6SfnBL4ZOh"
      },
      "source": [
        "Dummy Net with random policy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pTsrsjcExD1m"
      },
      "source": [
        "  class RandomNet(nn.Module):\n",
        "\n",
        "    def __init__(self,outputs):\n",
        "        super(RandomNet, self).__init__()        \n",
        "        self.output_size=outputs;\n",
        "        \n",
        " \n",
        "    def forward(self, x):\n",
        "        return  x\n",
        "\n",
        "    def policy(self,state):\n",
        "       return  torch.tensor([[random.randrange(self.output_size)]], device=device, dtype=torch.long)\n",
        "     \n",
        "    \n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5zH08XItLDFB"
      },
      "source": [
        "Parameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pZO6lE44LEn5"
      },
      "source": [
        " \n",
        "\n",
        " \n",
        " \n",
        "action_space    = [\n",
        "            (-1, 1, 0.2), (0, 1, 0.2), (1, 1, 0.2), #           Action Space Structure\n",
        "            (-1, 1,   0), (0, 1,   0), (1, 1,   0), #        (Steering Wheel, Gas, Break)\n",
        "            (-1, 0, 0.2), (0, 0, 0.2), (1, 0, 0.2), # Range        -1~1       0~1   0~1\n",
        "            (-1, 0,   0), (0, 0,   0), (1, 0,   0)];\n",
        "\n",
        " \n",
        "n_actions=len(action_space)\n",
        "\n",
        "#hyper-parameters\n",
        "TotalEpisodes=1001;\n",
        "MaxSteps=1000;\n",
        "ControlSteps=1; # number of iterations the same action is executed in the environment \n",
        "FrameStack=1 # number of consecutive frames used to represent the state\n",
        "FreezeCounter=25; # clone the model every X episodes\n",
        "BatchSize=64;\n",
        "exploration_threshold=1\n",
        "exploration_threshold_min=0.01\n",
        "exploration_decay=0.997\n",
        "discount_factor=0.95\n",
        "LearningRate=0.0001\n",
        " \n",
        "SaveAtCounter=50 # save model or video at every X episodes\n",
        "ResetCounter=50 # maximum number of experiences with bad performance\n",
        "InitCounter=20 # minimum number of experiences per episode\n",
        "FrameSize=3; # 3 if RGB 1 if grayscale\n",
        "\n",
        "\n",
        "# use these flags to enable Double DQN and Duelinng (Dueling DQN or Double Dueling DQN)\n",
        "usedoubleDQN=False\n",
        "useDueling=False\n",
        "\n",
        "UseClonedTrack=True # forces the same Track for all episodes"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M7SRgxRm5Ti-"
      },
      "source": [
        "Run the new netwwork with random policy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z0YCI9TL5Szv"
      },
      "source": [
        "randomnet=RandomNet(n_actions)\n",
        "createMovie(randomnet,\"random\",None,None,False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OsEv2cZLwy3e"
      },
      "source": [
        "Initialization and Parameters:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6yygleEXxEpG"
      },
      "source": [
        "env = gym.make(\"CarRacing-v0\")\n",
        "spec = gym.spec(\"CarRacing-v0\")\n",
        "\n",
        "\n",
        "#network DQN\n",
        "policy_net = DQN(FrameSize*FrameStack, n_actions).to(device)\n",
        "target_net = DQN(FrameSize*FrameStack, n_actions).to(device)\n",
        "target_net.load_state_dict(policy_net.state_dict())\n",
        "target_net.eval()\n",
        "\n",
        "buffer = ReplayBuffer(5000);  # max number of experiences in the buffer (may need to be smaller if CUDA memory errors occur).\n",
        "\n",
        "optimizer = torch.optim.Adam(policy_net.parameters(), lr=LearningRate)\n",
        "\n",
        "loss=  torch.nn.SmoothL1Loss()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VVf5wZAHxOST"
      },
      "source": [
        "DQN training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IPMrYDxyxVT1"
      },
      "source": [
        "def trainModel(buffer_data):\n",
        "    if buffer_data.counter < BatchSize:\n",
        "        return 0.0\n",
        "\n",
        "    state_batch, action_batch, reward_batch, new_state_batch, done_batch = buffer_data.sample_buffer(BatchSize)\n",
        "    state_batch = torch.stack(state_batch).to(device)\n",
        "    action_batch = torch.stack(action_batch).reshape(-1,1).to(device)\n",
        "    new_state_batch = torch.stack(new_state_batch).to(device)\n",
        "    reward_batch = torch.stack(reward_batch).reshape(-1,1).to(device)\n",
        "    done_batch = torch.stack(done_batch).reshape(-1,1).to(device)\n",
        "\n",
        " \n",
        "    if usedoubleDQN:\n",
        "     \n",
        "        #Add Double DQN code here\n",
        "\n",
        "    else: \n",
        "        q_actual = torch.gather(policy_net(state_batch),1,index=action_batch)\n",
        "        with torch.no_grad():\n",
        "            q_max_next, _ = target_net(new_state_batch).max(dim=1)\n",
        "            q_max_next=q_max_next.reshape(-1, 1)\n",
        "\n",
        "        q_target = (q_max_next * discount_factor)*(1-done_batch) + reward_batch\n",
        "        \n",
        "        ll=loss(q_actual, q_target)\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    ll.backward()\n",
        "    torch.nn.utils.clip_grad_norm_(policy_net.parameters(), 1)\n",
        "    optimizer.step()\n",
        "\n",
        "    return ll.item();"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SiLK6ZG3xlUd"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wv8KK14wxs5r"
      },
      "source": [
        "loss_val,scores, episodes,events, avg_scores,avg_scores100,exploration = [],[],[], [],[], [], []\n",
        "\n",
        "bestScore=-99999;\n",
        "at=0;\n",
        "bestNet=copy.deepcopy(policy_net);\n",
        "\n",
        "\n",
        "# only use one track \n",
        "new_state_INIT=env.reset()\n",
        "clonedtrack=copy.deepcopy(env.track);\n",
        "\n",
        "\n",
        "for f in range(TotalEpisodes):\n",
        "    done  = False\n",
        "    score = 0.0\n",
        "    tloss = 0.0\n",
        " \n",
        "    new_state=env.reset()\n",
        "\n",
        "    # disable to train using random tracks\n",
        "    if UseClonedTrack:\n",
        "       new_state=new_state_INIT;\n",
        "       env.track=copy.deepcopy(clonedtrack);\n",
        "    \n",
        " \n",
        "    stackedState=[np.rollaxis(new_state, 2, 0).copy() for i in range(FrameStack)];\n",
        "    state =torch.from_numpy(np.reshape(np.array(stackedState),(FrameSize*FrameStack,96,96)) ).float().to(device)\n",
        "   \n",
        "    if f % FreezeCounter == 0:\n",
        "       print(\"########################################\"+str(f)+\" of \"+str(TotalEpisodes))\n",
        "       target_net.load_state_dict(policy_net.state_dict())\n",
        "       \n",
        "    if f % SaveAtCounter == 0:\n",
        "      #torch.save(policy_net.state_dict(), \"v2CarRacing_\"+str(f)+'_model.ckpt')\n",
        "       createMovie(policy_net,\"CarRacing_\"+str(f),new_state_INIT,clonedtrack,UseClonedTrack)\n",
        "       \n",
        "    resetStatus=0;\n",
        "    i=0    \n",
        "    \n",
        "    for F in range(MaxSteps):\n",
        "        #env.render()\n",
        "        action = policy_net.getPolicy(state,exploration_threshold)\n",
        "        reward=0\n",
        "        for _ in range(ControlSteps):\n",
        "            new_state, r, done, info = env.step(action_space[action.item()])\n",
        "            reward+=r\n",
        "            i=i+1\n",
        "            if(done):\n",
        "                break\n",
        "        if reward<0 and i>InitCounter:\n",
        "            resetStatus=resetStatus+1;\n",
        "        else:\n",
        "            resetStatus=0\n",
        "            \n",
        "        stackedState.pop(0)\n",
        "        stackedState.append(np.rollaxis(new_state, 2, 0).copy())\n",
        "        new_state=torch.from_numpy(np.reshape(np.array(stackedState),(FrameSize*FrameStack,96,96))).float().to(device)\n",
        "        \n",
        "        score += reward\n",
        "        if(F<(MaxSteps-1)):\n",
        "            buffer.store_tuples(state, action, torch.tensor(reward), new_state, torch.tensor(int(done)))\n",
        "            \n",
        "        state = new_state\n",
        "        trainModel(buffer)\n",
        "\n",
        "\n",
        "        if(done or resetStatus>=ResetCounter):\n",
        "            break        \n",
        "    exploration_threshold= exploration_threshold*exploration_decay if exploration_threshold > exploration_threshold_min else exploration_threshold_min\n",
        "\n",
        "    if(score>bestScore):\n",
        "        print(score,F)\n",
        "        bestScore=score;\n",
        "        bestNet=copy.deepcopy(policy_net);\n",
        "        at=f;\n",
        "    \n",
        "    exploration.append(exploration_threshold)\n",
        "    scores.append(score)\n",
        "    episodes.append(f)\n",
        "    events.append(F)\n",
        "    avg_scores.append(score/F)\n",
        "    avg_scores100.append(np.mean(scores[-100:]))\n",
        "\n",
        "\n",
        "torch.save(bestNet.state_dict(), \"BestCarRacing_\"+str(f)+'_'+str(bestScore)+'_model.ckpt')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_qEpTPbGxu_M"
      },
      "source": [
        "Plot performance"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uWwFBt0vxxEd"
      },
      "source": [
        "figure(figsize=(12, 6), dpi=80)\n",
        "plt.plot(episodes, scores)\n",
        "plt.plot(episodes, events)\n",
        "plt.plot(episodes, avg_scores)\n",
        "plt.plot(episodes, avg_scores100)\n",
        "plt.plot(episodes, exploration)\n",
        "plt.xlabel('episodes')\n",
        "plt.ylabel('y axis label')\n",
        " \n",
        "plt.title('Track Locked DQN '+str(usedoubleDQN)+' '+str(useDueling))\n",
        "plt.legend(['scores', 'events', 'avg_scores', 'avg_scores100','exploration'])\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i7IDgBVCx3mZ"
      },
      "source": [
        "View Best Results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T9RpF49oOsZj"
      },
      "source": [
        "createMovie(bestNet,'bestNet',new_state_INIT,clonedtrack,UseClonedTrack)\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}